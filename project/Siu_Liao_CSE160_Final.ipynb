{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable intellisense\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-c6333b09c47b>, line 202)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-c6333b09c47b>\"\u001b[0;36m, line \u001b[0;32m202\u001b[0m\n\u001b[0;31m    ans_tagList =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Name:        module1\n",
    "# Purpose:\n",
    "#\n",
    "# Author:      Chianson Siu, Wei Liao\n",
    "#\n",
    "# Created:     13/02/2019\n",
    "#-------------------------------------------------------------------------------\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "def removeBodyXMLTags(df):\n",
    "    '''\n",
    "    This function removes all XML tags in the \"Body\" column of the pandas data\n",
    "    frame, e.g, all <p> and </p> tags should be removed.\n",
    "    @param df is a pandas dataframe with one column that refers to the\n",
    "              body of the text named \"Body\"\n",
    "    @return a pandas datafame    \n",
    "    '''\n",
    "\n",
    "    # For each post in column \"Body\" of the panda dataframe\n",
    "    # Remove all XML tags and put the result back to the\n",
    "    # corresponding cell\n",
    "    for i in range(len(df.index)):\n",
    "        # Obtain cell content to edit\n",
    "        postBody = df.at[i, \"Body\"]\n",
    "\n",
    "        # Remove XML flasg and put result back to corresponding column\n",
    "        df.at[i, \"Body\"] = removeXMLTags(str(postBody))\n",
    "    return df\n",
    "\n",
    "\n",
    "def removeXMLTags(text):\n",
    "    '''\n",
    "    Given a string of content, remove all occurences of XML tags.\n",
    "    (XML tags is anything that starts with '<' and ends with '>')\n",
    "\n",
    "    @param text String. A string of text.\n",
    "\n",
    "    @return a String with all occurences of XML tags removed\n",
    "    '''\n",
    "\n",
    "    textList = list(text)\n",
    "    # Remove All the XML tags\n",
    "    j = 0\n",
    "    while j < len(textList):\n",
    "        if textList[j] == \"<\":\n",
    "            while textList[j] != \">\":\n",
    "                textList.pop(j)\n",
    "            textList.pop(j)\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    # Convert list back into string and return the result\n",
    "    return ''.join(textList)\n",
    "\n",
    "\n",
    "\n",
    "def wordToCount(text):\n",
    "    '''\n",
    "    Given a string of content, count the number of occurence of each word,\n",
    "    then returns the result.\n",
    "    \n",
    "    @param text String. A string of text.\n",
    "\n",
    "    @return a dictionary mapping word to word count\n",
    "    '''\n",
    "    wordDict = dict()\n",
    "    textList = text.split(\" \")\n",
    "    for word in textList:\n",
    "        key = word.lower()\n",
    "        wordDict[key] = wordDict.get(key, 0) + 1\n",
    "    return wordDict\n",
    "\n",
    "\n",
    "def top10Words(wordDict):\n",
    "    '''\n",
    "    Given a dictionary that maps word to its occurence count, return a\n",
    "    list of top 10 highest occuring words\n",
    "\n",
    "    @param wordDict Dictionary. A dictionary that maps a word to the \n",
    "                    number of times that it occured in a text\n",
    "\n",
    "    @return a list of top 10 highest occuring words\n",
    "    '''\n",
    "    \n",
    "    # Create list of words to exclude from final list\n",
    "    exclude = set(stopwords.words('english'))\n",
    "\n",
    "    result = list()\n",
    "    # Sort dictionary into tuples according to value\n",
    "    # Creates a list of tuples (word, count) sorted from highest count\n",
    "    # to lowest count\n",
    "    sorted_words = sorted(wordDict.items(), key=operator.itemgetter(1),\n",
    "        reverse=True)\n",
    "\n",
    "    # Take only the top 10 word part of the sorted_words\n",
    "    # Exclude the common non-meaning words such as \"the\",\n",
    "    # \"of\", \"or\"\n",
    "    for i in range(100):\n",
    "        # Make sure i is not over the range of sorted_words\n",
    "        # and result has 10 words\n",
    "        if i >= len(sorted_words) or len(result) == 10:\n",
    "            break\n",
    "\n",
    "        # Append words that are not in the exclude list\n",
    "        if sorted_words[i][0] not in exclude:\n",
    "            result.append(sorted_words[i][0])\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter10User(users_df):\n",
    "    '''\n",
    "    Takes a user data frame. Filters out the first 10 users and store the\n",
    "    results in a dictionary with key being the user ID and the value being\n",
    "    the user's reputation score\n",
    "\n",
    "    @param users_df Pandas datagrame. The datafram that contains information\n",
    "                    on users.\n",
    "\n",
    "    @return a dictionary of the first 10 users (key is user ID, value is\n",
    "            reputation)\n",
    "    ''' \n",
    "    result = dict()\n",
    "    for i in range(10):\n",
    "        # Make user dataframe has at least 10 entries\n",
    "        if len(users_df.index) > 10:\n",
    "            # Take the first 10 Id and Reputation out\n",
    "            section = users_df.head(n=10)[[\"_Id\", \"_Reputation\"]]\n",
    "            # Store ID as key and Reputation as value\n",
    "            result[section.iloc[i, 0]] = section.iloc[i, 1]\n",
    "    return result\n",
    "\n",
    "\n",
    "def numWords(text):\n",
    "\t'''\n",
    "\tCount the number of words there is in a block of string\n",
    "\t'''\n",
    "\tcount = 0\n",
    "\tfor word in text:\n",
    "\t\tcount += 1\n",
    "\treturn count\n",
    "\n",
    "def make(post_df):\n",
    "\n",
    "\tres_df = DataFrame(columns=('UserID', 'AnsCount', 'AnsAverageLen', 'Reputation'))\n",
    "\n",
    "\tfor i in range(len(post_df.index)):\n",
    "\t\tif posts_df.at[i, \"OwnerId\"] in posts_df[ : , \"AcceptedAnswerId\"]:\n",
    "\n",
    "\t\t\tres_df\n",
    "\n",
    "\t# iterate through post, \n",
    "\t# if postID is in acceptedAnswerID list,\n",
    "\t#    put OwnerUserID, Body word count in result table\n",
    "\t#    update number of answer in result table\n",
    "\t# if post is this aaID then get OwnerUserID and Body (count)\n",
    "\n",
    "\n",
    "def q1():\n",
    "    '''\n",
    "    Performs tasks necessary to solve research question 1\n",
    "    '''\n",
    "\n",
    "    # Create dataframe\n",
    "    ans_df = pd.DataFrame(columns=('ID', 'Tags', 'Top10Body',\n",
    "        'Length', 'Title'))\n",
    "    \n",
    "    noAns_df = pd.DataFrame(columns=('ID', 'Tags', 'Top10Body',\n",
    "        'Length', 'Title'))\n",
    "\n",
    "    # read in the CSV file to pandas dataframe. The CSV file must be coded as\n",
    "    # UTF-8.\n",
    "    posts_df = pd.read_csv(\"Posts.csv\")\n",
    "    posts_df = removeBodyXMLTags(posts_df)\n",
    "\n",
    "    # Fill in PostID, Length, AnswerCount of each row\n",
    "    for i in range(len(posts_df.index)):\n",
    "        # obtain body of the post\n",
    "        body = posts_df.at[i, \"Body\"]\n",
    "\n",
    "        # Obain each column info\n",
    "        ID = posts_df.at[i, \"Id\"]\n",
    "        tags = posts_df.at[i, \"Tags\"]\n",
    "        top10Body = top10Words(wordToCount(body))\n",
    "        length = len(str(body))\n",
    "        title = posts_df.at[i, \"Title\"]\n",
    "\n",
    "        # Add a new row in the ans or no_ans dataframe with the correct values\n",
    "        if pd.isnull(posts_df.loc[i, \"AcceptedAnswerId\"]):\n",
    "            noAns_df.loc[i] = [ID, tags, top10Body, length, title]\n",
    "        else:\n",
    "            ans_df.loc[i] = [ID, tags, top10Body, length, title]\n",
    "            \n",
    "    \n",
    "    # Find Average Word Count for ans and noAns\n",
    "    ans_averageW = float(ans_df.mean(axis=4))\n",
    "    noAns_averageW = float(noAns_df.mean(axis=4))\n",
    "    \n",
    "    # Find top 5 tags for ans and noAns\n",
    "#    ans_tagList = \n",
    "#    noAns_tagList\n",
    "    \n",
    "    # Find top 10 keywords on title for ans and noAns\n",
    "    \n",
    "    # Find top 10 keywords body for ans and noAns\n",
    "    \n",
    "    print(\"noAns_df: \")\n",
    "    print(noAns_df)\n",
    "    print(\"ans_df:\")\n",
    "    print(ans_df)\n",
    "\n",
    "\n",
    "def q2(users_df):\n",
    "    '''\n",
    "    Performs tasks necessary to solve research question 2\n",
    "    '''\n",
    "    q2_df = pd.DataFrame(columns=(\"UserID\", \"Reputation\", \"AnswerLength\",\n",
    "        \"PostLength\", \"CommentLength\", \"AnswerKeyWord\", \"PostKeyWord\",\n",
    "        \"CommentKeyword\", \"QuestionKeyword\"))\n",
    "\n",
    "    # Sort user by highest reputation\n",
    "    sorted_df = users_df.sort_values(\"_Reputation\", ascending=False)\n",
    "    user_rep_dict = filter10User(sorted_df)\n",
    "\n",
    "\n",
    "def q3(users_df):\n",
    "    '''\n",
    "    Performs tasks necessary to solve research question 3\n",
    "    '''\n",
    "    q3_df = pd.DataFrame(columns=(\"UserID\", \"Reputation\", \"AnswerLength\",\n",
    "        \"PostLength\", \"CommentLength\", \"AnswerKeyWord\", \"PostKeyWord\",\n",
    "        \"CommentKeyword\", \"QuestionKeyword\"))\n",
    "\n",
    "    # Sort user by lowest reputation\n",
    "    sorted_df = users_df.sort_values(\"_Reputation\")\n",
    "    user_rep_dict = filter10User(sorted_df)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Research Question 1 Analysis DONE!\n",
    "    #q1()\n",
    "\n",
    "    users_df = pd.read_csv(\"Users.csv\")\n",
    "\n",
    "    # Research Question 2 Analysis\n",
    "    #q2(users_df)\n",
    "\n",
    "    # Research Question 3 Analysis\n",
    "    #q3(users_df)\n",
    "\n",
    "  #  df = pd.read_csv(\"Comments.csv\")\n",
    "   # print(df.loc[0, :])\n",
    "    # tags_df = pd.read_csv(\"Tags.csv\")\n",
    "    #print(tags_df)\n",
    "\n",
    "\n",
    "    # test num_words()\n",
    "\n",
    "# If this file is run as a Python script (such as by typing\n",
    "# \"python tests.py\" at the command shell), then run the following:\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/user/nltk_data'\n    - '/Users/user/anaconda3/nltk_data'\n    - '/Users/user/anaconda3/share/nltk_data'\n    - '/Users/user/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/user/nltk_data'\n    - '/Users/user/anaconda3/nltk_data'\n    - '/Users/user/anaconda3/share/nltk_data'\n    - '/Users/user/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-816bb21d6ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-828e1ce67028>\u001b[0m in \u001b[0;36mq1\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tags\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtop10Body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop10Words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordToCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-828e1ce67028>\u001b[0m in \u001b[0;36mtop10Words\u001b[0;34m(wordDict)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Create list of words to exclude from final list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/user/nltk_data'\n    - '/Users/user/anaconda3/nltk_data'\n    - '/Users/user/anaconda3/share/nltk_data'\n    - '/Users/user/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "q1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
